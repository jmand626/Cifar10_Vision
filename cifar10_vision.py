# -*- coding: utf-8 -*-
"""Joban Mand - hw4-a3-cifar-image-classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q7qPz-Z3ZVJy9WRDWQkNimCILDfjdWnc

# Homework 4: Image Classification on CIFAR-10
  🛫 🚘 🐦 🐱 🦌 🐶 🐸 🐴 🚢 🛻

## Information before starting

In this problem, we will explore different deep learning architectures for image classification on the CIFAR-10 dataset. Make sure that you are familiar with torch `Tensor`s, two-dimensional convolutions (`nn.Conv2d`) and fully-connected layers (`nn.Linear`), ReLU non-linearities (`F.relu`), pooling (`nn.MaxPool2d`), and tensor reshaping (`view`). **Make sure to read through all instructions in both this notebook and in the PDF while completing this problem!**

### Copying this Colab Notebook to your Google Drive

Since the course staff is the author of this notebook, you cannot make any lasting changes to it. You should make a copy of it to your Google Drive by clicking **File -> Save a Copy in Drive**.

### Problem Introduction

You've already had some practice using the PyTorch library in HW3, but this problem dives into training more complex deep learning models.

The specific task we are trying to solve in this problem is image classification. We're using a common dataset called CIFAR-10 which has 60,000 images separated into 10 classes:
* airplane
* automobile
* bird
* cat
* deer
* dog
* frog
* horse
* ship
* truck

We've provided an end-to-end example of loading data, training a model, and performing evaluation. We recommend using this code as a template for your implementations of the more complex models. Feel free to modify or reuse any of the functions we provide.

**Unlike other coding problems in the past, this one does not include an autograded component.**

### Enabling GPU

We are using Google Colab because it has free GPU runtimes available. GPUs can accelerate training times for this problem by 10-100x when compared to using CPU. To use the GPU runtime on Colab, make sure to **enable** the runtime by going to **Runtime -> Change runtime type -> Select T4 GPU under "Hardware accelerator"**.

Note that GPU runtimes are *limited* on Colab. We recommend limiting your training to short-running jobs (under 15 minutes each) and spread your work over time, if possible. Colab *will* limit your usage of GPU time, so plan ahead and be prepared to take breaks during training. If you have used up your quota for GPU, check back in a day or so to be able to enable GPU again.

Your code will still run on CPU, so if you are just starting to implement your code or have been GPU limited by Colab, you can still make changes and run your code - it will just be quite a bit slower. You can also choose to download your notebook and run locally if you have a personal GPU or have a faster CPU than the one Colab provides. If you choose to do this, you may need to install the packages that this notebook depends on to your `cse446` conda environment or to another Python environment of your choice.

To check if you have enabled GPU, run the following cell. If `device` is `cuda`, it means that GPU has been enabled successfully.
"""

import torch

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(DEVICE)  # this should print out CUDA

"""### Submitting your assignment

Once you are done with the problem, make sure to put all of your necessary figures into your PDF submission. Then, download this notebook as a Python file (`.py`) by going to **File -> Download -> Download `.py`**. Rename this file as `hw4-a3.py` and upload to the Gradescope submission for HW4 code.

## End-to-end Example

### Background and Setup

1. We first import all of the dependencies required for this problem:
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
from torch import nn
import numpy as np

from typing import Tuple, Union, List, Callable
from torch.optim import SGD
import torchvision
from torch.utils.data import DataLoader, TensorDataset, random_split
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

# %matplotlib inline

"""2. And check if we are using GPU, if it is available. (Make sure to set your runtime to enable GPU!)"""

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(DEVICE)  # this should print out CUDA

"""*To use the GPU, you will need to send both the model and the data to a device; this transfers the model from its default location on CPU to the GPU.*

*Note that torch operations on Tensors will fail if they are not located on the same device. Here's a small example of how to send the model and data to your device:*

  ```python
  model = model.to(DEVICE)  # Sending a model to GPU

  for x, y in tqdm(data_loader):
    x, y = x.to(DEVICE), y.to(DEVICE)
  ```
*When reading tensors you may need to send them back to cpu, you can do so with `x = x.cpu()`*

3. Now, let's load the CIFAR-10 data. We can take advantage of public datasets available through PyTorch torchvision!
"""

train_dataset = torchvision.datasets.CIFAR10("./data", train=True, download=True, transform=torchvision.transforms.ToTensor())
test_dataset = torchvision.datasets.CIFAR10("./data", train=False, download=True, transform=torchvision.transforms.ToTensor())

"""4. Finally, like we did in HW3, we'll use the PyTorch `DataLoader` to wrap our datasets. You've already seen that `DataLoader`s handle batching, shuffling, and iterating over data, and this is really useful for this problem as well!

*Since training on all of the 50,000 training samples can be prohibitively expensive, we define a flag called* `SAMPLE_DATA` *that controls if we should make the dataset smaller for faster training time. When* `SAMPLE_DATA=true`, *we'll only use 10% of our training data when training and performing our hyperparameter searches.*  **Make sure that you've set `SAMPLE_DATA=false` when you want to perform your final training loops for submission!**
"""

SAMPLE_DATA = False # set this to True if you want to speed up training when searching for hyperparameters!

batch_size = 128

if SAMPLE_DATA:
  train_dataset, _ = random_split(train_dataset, [int(0.1 * len(train_dataset)), int(0.9 * len(train_dataset))]) # get 10% of train dataset and "throw away" the other 90%

train_dataset, val_dataset = random_split(train_dataset, [int(0.9 * len(train_dataset)), int( 0.1 * len(train_dataset))])

# Create separate dataloaders for the train, test, and validation set
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    shuffle=True
)

"""Now, we're ready to train!

### Logistic Regression Example

Let's first take a look at our data to get an understanding of what we are doing. As a reminder, CIFAR-10 is a dataset containing images split into 10 classes.
"""

imgs, labels = next(iter(train_loader))
print(f"A single batch of images has shape: {imgs.size()}")
example_image, example_label = imgs[0], labels[0]
c, w, h = example_image.size()
print(f"A single RGB image has {c} channels, width {w}, and height {h}.")

# This is one way to flatten our images
batch_flat_view = imgs.view(-1, c * w * h)
print(f"Size of a batch of images flattened with view: {batch_flat_view.size()}")

# This is another equivalent way
batch_flat_flatten = imgs.flatten(1)
print(f"Size of a batch of images flattened with flatten: {batch_flat_flatten.size()}")

# The new dimension is just the product of the ones we flattened
d = example_image.flatten().size()[0]
print(c * w * h == d)

# View the image
t =  torchvision.transforms.ToPILImage()
plt.imshow(t(example_image))

# These are what the class labels in CIFAR-10 represent. For more information,
# visit https://www.cs.toronto.edu/~kriz/cifar.html
classes = ["airplane", "automobile", "bird", "cat", "deer", "dog", "frog",
           "horse", "ship", "truck"]
print(f"This image is labeled as class {classes[example_label]}")

"""In this problem, we will attempt to predict what class an image is labeled as.

1. First, let's create our model. Note: for a linear model we could flatten the data before passing it into the model, but that is not the case for convolutional neural networks.
"""

def linear_model() -> nn.Module:
    """Instantiate a linear model and send it to device."""
    model =  nn.Sequential(
            nn.Flatten(),
            nn.Linear(d, 10)
         )
    return model.to(DEVICE)

"""2. Let's define a method to train this model using SGD as our optimizer."""

def train(
    model: nn.Module, optimizer: SGD,
    train_loader: DataLoader, val_loader: DataLoader,
    epochs: int = 20
    )-> Tuple[List[float], List[float], List[float], List[float]]:
    """
    Trains a model for the specified number of epochs using the loaders.

    Returns:
    Lists of training loss, training accuracy, validation loss, validation accuracy for each epoch.
    """

    loss = nn.CrossEntropyLoss()
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []
    for e in tqdm(range(epochs)):
        model.train()
        train_loss = 0.0
        train_acc = 0.0

        # Main training loop; iterate over train_loader. The loop
        # terminates when the train loader finishes iterating, which is one epoch.
        for (x_batch, labels) in train_loader:
            x_batch, labels = x_batch.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()
            labels_pred = model(x_batch)
            batch_loss = loss(labels_pred, labels)
            train_loss = train_loss + batch_loss.item()

            labels_pred_max = torch.argmax(labels_pred, 1)
            batch_acc = torch.sum(labels_pred_max == labels)
            train_acc = train_acc + batch_acc.item()

            batch_loss.backward()
            optimizer.step()
        train_losses.append(train_loss / len(train_loader))
        train_accuracies.append(train_acc / (batch_size * len(train_loader)))

        # Validation loop; use .no_grad() context manager to save memory.
        model.eval()
        val_loss = 0.0
        val_acc = 0.0

        with torch.no_grad():
            for (v_batch, labels) in val_loader:
                v_batch, labels = v_batch.to(DEVICE), labels.to(DEVICE)
                labels_pred = model(v_batch)
                v_batch_loss = loss(labels_pred, labels)
                val_loss = val_loss + v_batch_loss.item()

                v_pred_max = torch.argmax(labels_pred, 1)
                batch_acc = torch.sum(v_pred_max == labels)
                val_acc = val_acc + batch_acc.item()
            val_losses.append(val_loss / len(val_loader))
            val_accuracies.append(val_acc / (batch_size * len(val_loader)))

    return train_losses, train_accuracies, val_losses, val_accuracies

"""3. Now, let's define our hyperparameter search. For this problem, we will be using SGD. The two hyperparameters for our linear model trained with SGD are the learning rate and momentum. Only learning rate will be searched for in this example, but you will be tuning multiple hyperparameters. **Feel free to experiment with hyperparameters and how you search. We recommend implementing random search!**

*Note: We ask you to plot the accuracies for the 3 best models for each structure, so you will need to return multiple sets of hyperparameters for the homework.*
"""

def parameter_search(train_loader: DataLoader,
                     val_loader: DataLoader,
                     model_fn:Callable[[], nn.Module]) -> float:
    """
    Parameter search for our linear model using SGD.

    Args:
    train_loader: the train dataloader.
    val_loader: the validation dataloader.
    model_fn: a function that, when called, returns a torch.nn.Module.

    Returns:
    The learning rate with the least validation loss.
    NOTE: you may need to modify this function to search over and return
     other parameters beyond learning rate.
    """
    num_iter = 10
    best_loss = torch.tensor(np.inf)
    best_lr = 0.0

    lrs = torch.linspace(10 ** (-6), 10 ** (-1), num_iter)

    for lr in lrs:
        print(f"trying learning rate {lr}")
        model = model_fn()
        optim = SGD(model.parameters(), lr)
        train_loss, train_acc, val_loss, val_acc = train(
            model,
            optim,
            train_loader,
            val_loader,
            epochs=20
            )

        if min(val_loss) < best_loss:
            best_loss = min(val_loss)
            best_lr = lr

    return best_lr

"""4. Now that we have everything, we can train and evaluate our model."""

best_lr = parameter_search(train_loader, val_loader, linear_model)

model = linear_model()
optimizer = SGD(model.parameters(), best_lr)

# We are using 20 epochs for this example. You may have to use more.
train_loss, train_accuracy, val_loss, val_accuracy = train(
    model, optimizer, train_loader, val_loader, 20)

"""5. We can also plot the training and validation accuracy for each epoch."""

epochs = range(1, 21)
plt.plot(epochs, train_accuracy, label="Train Accuracy")
plt.plot(epochs, val_accuracy, label="Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Logistic Regression Accuracy for CIFAR-10 vs Epoch")
plt.show()

"""The last thing we have to do is evaluate our model on the testing data."""

def evaluate(
    model: nn.Module, loader: DataLoader
) -> Tuple[float, float]:
    """Computes test loss and accuracy of model on loader."""
    loss = nn.CrossEntropyLoss()
    model.eval()
    test_loss = 0.0
    test_acc = 0.0
    with torch.no_grad():
        for (batch, labels) in loader:
            batch, labels = batch.to(DEVICE), labels.to(DEVICE)
            y_batch_pred = model(batch)
            batch_loss = loss(y_batch_pred, labels)
            test_loss = test_loss + batch_loss.item()

            pred_max = torch.argmax(y_batch_pred, 1)
            batch_acc = torch.sum(pred_max == labels)
            test_acc = test_acc + batch_acc.item()
        test_loss = test_loss / len(loader)
        test_acc = test_acc / (batch_size * len(loader))
        return test_loss, test_acc

test_loss, test_acc = evaluate(model, test_loader)
print(f"Test Accuracy: {test_acc}")

"""The rest is yours to code. You can structure the code any way you would like.

We do advise making using code cells and functions (train, search, predict etc.) for each subproblem, since they will make your code easier to debug.

Also note that several of the functions above can be reused for the various different models you will implement for this problem; i.e., you won't need to write a new `evaluate()`.

## Your Turn!

The rest is yours to code. You are welcome to structure the code any way you would like.

We do advise making using code cells and functions (train, search, predict etc.) for each subproblem, since they will make your code easier to debug.

Also note that several of the functions above can be reused for the various different models you will implement for this problem; i.e., you won't need to write a new `evaluate()`. Before you reuse functions though, make sure they are compatible with what the assignment is asking for.

### Submitting Code

And as a last reminder, once you are done with the problem, make sure to put all of your necessary figures into your PDF submission. Then, download this notebook as a Python file (`.py`) by going to **File -> Download -> Download `.py`**. Rename this file as `hw4-a3.py` and upload to the Gradescope submission for HW4 code.

Whoever decided to make this assignment this unbelievably long needs therapy
"""

# Commented out IPython magic to ensure Python compatibility.
# Your code goes here...
import torch
from torch import nn
import numpy as np

import random

from typing import Tuple, Union, List, Callable, Any, Dict, Iterable, Iterator
from torch.optim import SGD
import torchvision
from torch.utils.data import DataLoader, TensorDataset, random_split, Subset
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

# %matplotlib inline

import torch

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(DEVICE)  # this should print out CUDA

# I had to come back and normalize. These values are extremely arbitary, because they have already been calculated. Asked to Yiping Wang on his Thursday 5/29 OH, he said it was ok. It does improve the accuracy
transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])
    # https://github.com/mingkai-zheng/ReSSL/issues/5
])

full_train_dataset = torchvision.datasets.CIFAR10("./data", train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.CIFAR10("./data", train=False, download=True, transform=transform)

# Unormalized version here
#full_train_dataset = torchvision.datasets.CIFAR10("./data", train=True, download=True, transform=torchvision.transforms.ToTensor())
#test_dataset = torchvision.datasets.CIFAR10("./data", train=False, download=True, transform=torchvision.transforms.ToTensor())

# Define classes for CIFAR-10 (as in example)
classes = ["airplane", "automobile", "bird", "cat", "deer", "dog", "frog",
           "horse", "ship", "truck"]

"""Configuration residentSleeper. Thankfully so much of this assignment is just copying code from the example"""

SAMPLE_DATA = False # set this to True if you want to speed up training when searching for hyperparameters!
# As of the first 5/30 OH with Donovan, setting this to false is the play, otherwise it never gets there, honestly a noob trap

batch_size = 128
EPOCHS_FINALTRAINING = 18
# Once we get three top configurations, we also need to test it
# "so feel free to train for just a dozen or so epochs"
EPOCHS_HYPERPARAMETER_SEARCH = 15
# "Train for fewer epochs during hyperparameter search; usually, the “best” hyperparameter configurations will be the best at every stopping point for this problem"
train_dataset = full_train_dataset
print(f"{len(full_train_dataset)}")

# Thankfully we can copy some stuff from above code
if SAMPLE_DATA:
  train_dataset, _ = random_split(train_dataset, [int(0.1 * len(train_dataset)), int(0.9 * len(train_dataset))])

train_dataset, val_dataset = random_split(train_dataset, [int(0.9 * len(train_dataset)), int( 0.1 * len(train_dataset))])
print(f"{len(train_dataset)}")
print(f"{len(val_dataset)}")
print(f"{len(test_dataset)}")
# Test dataset is never affected by the SAMPLE_DATA flag!

# Create separate dataloaders for the train, test, and validation set
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    shuffle=True
)

# Finally flatten our images
imgs, labels = next(iter(train_loader))
print(f"A single batch of images has shape: {imgs.size()}")
example_image, example_label = imgs[0], labels[0]
c, w, h = example_image.size()
print(f"A single RGB image has {c} channels, width {w}, and height {h}.")

# This is one way to flatten our images
batch_flat_view = imgs.view(-1, c * w * h)
print(f"Size of a batch of images flattened with view: {batch_flat_view.size()}")

# The new dimension is just the product of the ones we flattened
d = example_image.flatten().size()[0]
print(c * w * h == d)

# Reuse the train function
def train(
    model: nn.Module, optimizer: SGD,
    train_loader: DataLoader, val_loader: DataLoader,
    epochs: int = 20
    )-> Tuple[List[float], List[float], List[float], List[float]]:
    """
    Trains a model for the specified number of epochs using the loaders.

    Returns:
    Lists of training loss, training accuracy, validation loss, validation accuracy for each epoch.
    """

    loss = nn.CrossEntropyLoss()
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []
    for e in tqdm(range(epochs)):
        model.train()
        train_loss = 0.0
        train_acc = 0.0

        # Main training loop; iterate over train_loader. The loop
        # terminates when the train loader finishes iterating, which is one epoch.
        for (x_batch, labels) in train_loader:
            x_batch, labels = x_batch.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()
            labels_pred = model(x_batch)
            batch_loss = loss(labels_pred, labels)
            train_loss = train_loss + batch_loss.item()

            labels_pred_max = torch.argmax(labels_pred, 1)
            batch_acc = torch.sum(labels_pred_max == labels)
            train_acc = train_acc + batch_acc.item()

            batch_loss.backward()
            optimizer.step()
        train_losses.append(train_loss / len(train_loader))
        train_accuracies.append(train_acc / (batch_size * len(train_loader)))

        # Validation loop; use .no_grad() context manager to save memory.
        model.eval()
        val_loss = 0.0
        val_acc = 0.0

        with torch.no_grad():
            for (v_batch, labels) in val_loader:
                v_batch, labels = v_batch.to(DEVICE), labels.to(DEVICE)
                labels_pred = model(v_batch)
                v_batch_loss = loss(labels_pred, labels)
                val_loss = val_loss + v_batch_loss.item()

                v_pred_max = torch.argmax(labels_pred, 1)
                batch_acc = torch.sum(v_pred_max == labels)
                val_acc = val_acc + batch_acc.item()
            val_losses.append(val_loss / len(val_loader))
            val_accuracies.append(val_acc / (batch_size * len(val_loader)))

        # "At the end of each epoch (one pass over the training data), compute and print the training and validation
        # classification accuracy."
        print(f"Epoch {e+1} - Train Acc: {train_accuracies[-1]:.3f}, Val Acc: {val_accuracies[-1]:.3f}")

    return train_losses, train_accuracies, val_losses, val_accuracies

# Reuse the eval function
def evaluate(
    model: nn.Module, loader: DataLoader
) -> Tuple[float, float]:
    """Computes test loss and accuracy of model on loader."""
    loss = nn.CrossEntropyLoss()
    model.eval()
    test_loss = 0.0
    test_acc = 0.0
    with torch.no_grad():
        for (batch, labels) in loader:
            batch, labels = batch.to(DEVICE), labels.to(DEVICE)
            y_batch_pred = model(batch)
            batch_loss = loss(y_batch_pred, labels)
            test_loss = test_loss + batch_loss.item()

            pred_max = torch.argmax(y_batch_pred, 1)
            batch_acc = torch.sum(pred_max == labels)
            test_acc = test_acc + batch_acc.item()
        test_loss = test_loss / len(loader)
        test_acc = test_acc / (batch_size * len(loader))
        return test_loss, test_acc

"""Training Results Helper Function"""

def plot_training_results(
    top_results: List[Dict[str, Any]],
    model_name: str,
    target_accuracy: float):

    plt.figure()


    training_plot = range(1, len(top_results[0]['val_accuracies'])+1)
    for i,results in enumerate(top_results):
        # Run over important parameters for the plots. These tell us what function/model we are looking at

        # We specifically add these paramaters as they are the most fundamental pieces of the system - that is, m_a is for part a, and m_b is for part b
        summary_model = f"LR:{results['params']['lr']:.1e}"
        if 'M_a' in results['params']: summary_model +=f",M_a:{results['params']['M_a']}"
        if 'momentum' in results['params']: summary_model +=f",Mom:{results['params']['momentum']:.1e}"

        if 'M_b' in results['params']: summary_model += f", M_b:{results['params']['M_b']}"
        if 'k' in results['params']: summary_model += f", Filt:{results['params']['k']}"
        if 'N' in results['params']: summary_model += f", Filt:{results['params']['N']}"


        if 'weight_decay' in results['params']: summary_model += f", WD:{results['params']['weight_decay']:.1e}"

        plt.plot(training_plot, results['train_accuracies'], label=f"Train Config {i+1} ({summary_model})")
        plt.plot(training_plot, results['val_accuracies'], label=f"Val Config {i+1} ({summary_model})", linestyle='dotted')

    plt.axhline(y=target_accuracy, color='r', linestyle=':',label=f"{target_accuracy*100:.2f}% Target")
    # Horizontal line at either the 50 or 65% line based on parameter
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title(f"{model_name} accuracy vs epochs")
    plt.legend()
    plt.grid(True)
    plt.show()

"""Finally, Part A:"""

def get_parta_model(input_dim: int, M_a: int, output_dim: int = 10) -> nn.Module:
    # We must flatten at the begining of our model, like in the examples
    return nn.Sequential(
        # Thankfully this model definiton is much easier than the one in part b
        nn.Flatten(),
        nn.Linear(input_dim, M_a),
        nn.ReLU(),
        #nn.Dropout(p=0.6),
        nn.Linear(M_a, output_dim)
    ).to(DEVICE)

# Setup parameter search function
def test_parameters_partA(iterations: int = 10, epochs_count: int = EPOCHS_HYPERPARAMETER_SEARCH) -> List[Dict[str, Any]]:

    # This is where we lay down the different hyperparameters to search over.
    # Obviously we will use random which is imported here
    history = []

    # Parameters to search over!

    # lr_array = [10e-4, 10e-3, 20e-3, 10e-2, 20e-2]
    # Orignally had that but it seemed like the lr was jut gneerally too high
    # lr_array = [0.0001, 0.001, 0.01, 0.1]
    # 10e-4 - 10e-2 log space
    lr_array = np.logspace(-4, -2, num=1000)
    lr = 0

    # "where M will be a hyperparameter you choose (M could be in the hundreds)."
    M_a = 0
    # Range - 200 - 1500

    # Range of 0.7 - 0.99
    momentum = 0

    weight_decay = 0
    weight_decay_array = np.logspace(-4, -2, num=1000)

    #best_loss = 0
    # Why on earth the example finds the minimum loss while they want us to find the maximum accuracy instead is beyond me
    best_accuracy = 0

    for i in range(iterations):
        lr = random.choice(lr_array)
        M_a = (int) (random.uniform(200, 1500))
        momentum = random.uniform(0.7, 0.99)

        # It seemed like my model was overfitting, so added l2 regularization!
        weight_decay = random.choice(weight_decay_array)

        model = get_parta_model(input_dim=d, M_a=M_a)
        #optimizer = SGD(model.parameters(), lr, momentum)
        optimizer = SGD(model.parameters(), lr, momentum, weight_decay=weight_decay)
        # Instantiate config


        #print(f"Config {i+1} lr={lr:.3e}, M_a={M_a}, mom={momentum}")
        print(f"Config {i+1} lr={lr:.3e}, M_a={M_a}, mom={momentum}, wd={weight_decay}")

        train_loss, train_acc, val_loss, val_acc = train(
            model,
            optimizer,
            train_loader,
            val_loader,
            epochs=epochs_count
        )

        if max(val_acc) > best_accuracy:
            best_accuracy = max(val_acc)

        #history.append({'params': {'lr': lr, 'momentum': momentum, 'M_a': M_a}, 'train_accuracies': train_acc, 'val_accuracies': val_acc, "cur_max_val_acc": best_accuracy})
        history.append({'params': {'lr': lr, 'momentum': momentum, 'M_a': M_a, 'weight_decay': weight_decay}, 'train_accuracies': train_acc, 'val_accuracies': val_acc, "cur_max_val_acc": best_accuracy})

    # Use the basic dictionary lambda/key sort feature
    history.sort(key=lambda x: x['cur_max_val_acc'], reverse=True)

    print("Top 3 part a models")
    for pos,config in enumerate(history[:3]):
        print(f" {pos+1}: {config['params']}, Val Acc: {config['cur_max_val_acc']:.3f}")

    return history[:3]

# Start running Part A code
iterations_count = 6
# "Use a coarser search space: If you are testing 100 different model configurations, it’s possible that it’s unnecessary, especially when you use momentum."

top_3 = test_parameters_partA(iterations_count, EPOCHS_HYPERPARAMETER_SEARCH)

if top_3:
  plot_training_results(top_3, "Part A", 0.50)
  # Now we will run it over the ENTIRE test set
  best_parta_result = top_3[0]
  best_parta_params = best_parta_result['params']

  print(f"Best Config: {best_parta_params}")
  print(f"Best Config Val Acc: {best_parta_result['cur_max_val_acc']:.3f}")


  model = get_parta_model(d, best_parta_params['M_a'])
  #optimizer = SGD(model.parameters(), lr=best_parta_params['lr'], momentum=best_parta_params['momentum'])
  optimizer = SGD(model.parameters(), lr=best_parta_params['lr'], momentum=best_parta_params['momentum'], weight_decay=best_parta_params['weight_decay'])
  train(model, optimizer, train_loader, val_loader, EPOCHS_FINALTRAINING)
  test_loss, test_acc = evaluate(model, test_loader)
  # Retrain fully!
  print(f"Part A - Best Config Test Acc: {test_acc:.3f}")

else:
  print("Check your code, no results")

"""**Part B Code!**"""

def get_partb_model(input_dim: int, M_b: int, k: int, N: int, img_size: int, output_dim: int = 10) -> nn.Module:
    # Conv size formula - (n - f - 2p)/s + 1
    # But in the case of the first convolutiuonal layer there is no padding and stride!!
    conv_out_dim = (img_size-k) + 1
    #pool_out_dim = ((conv_out_dim - 2) / 2) + 1
    # Cannot do that in the linear layer as it will be a float, must do integer division instead to approximate!
    pool_out_dim = ((conv_out_dim - N) // N) + 1
    # We have padding 0

    return nn.Sequential(
        # Following the instructions
        nn.Conv2d(input_dim, M_b, k),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=N, stride=N),
        nn.Flatten(),
        nn.Linear(M_b * (pool_out_dim ** 2), output_dim)
        # Applying m times the square of the pool as we have a fully connected layer where W2 is in R^{M * (something)^2}
    ).to(DEVICE)

# Setup parameter search function
def test_parameters_partB(iterations: int = 10, epochs_count: int = EPOCHS_HYPERPARAMETER_SEARCH) -> List[Dict[str, Any]]:

    # This is where we lay down the different hyperparameters to search over.
    # Obviously we will use random which is imported here
    history = []

    # Parameters to search over!

    # lr_array = [10e-4, 10e-3, 20e-3, 10e-2, 20e-2]
    # Orignally had that but it seemed like the lr was jut gneerally too high
    # lr_array = [0.0001, 0.001, 0.01, 0.1]
    # 10e-4 - 10e-2 log space
    lr_array = np.logspace(-4, -2, num=1000)
    lr = 0

    # "where M will be a hyperparameter you choose (M could be in the hundreds)."
    M_b = 0
    # Range - 200 - 1500

    # Range of 0.7 - 0.99
    momentum = 0

    weight_decay = 0
    weight_decay_array = np.logspace(-4, -2, num=1000)

    k_size_options = [3, 5, 7]
    N_pool_options = [2, 3, 4, 5, 6, 7, 10, 14]

    #best_loss = 0
    # Why on earth the example finds the minimum loss while they want us to find the maximum accuracy instead is beyond me
    best_accuracy = 0

    for i in range(iterations):
        lr = random.choice(lr_array)
        M_b = (int) (random.uniform(200, 1500))
        momentum = random.uniform(0.7, 0.99)
        k = random.choice(k_size_options)
        N = random.choice(N_pool_options)

        # It seemed like my model was overfitting, so added l2 regularization!
        weight_decay = random.choice(weight_decay_array)

        model = get_partb_model(input_dim=c, M_b=M_b, k=k, N=N, img_size=w, output_dim=10)

        optimizer = SGD(model.parameters(), lr, momentum, weight_decay=weight_decay)
        # Instantiate config

        print(f"Config {i+1} lr={lr:.3e}, M_b={M_b}, mom={momentum}, wd={weight_decay}, k={k}, N={N}")

        train_loss, train_acc, val_loss, val_acc = train(
            model,
            optimizer,
            train_loader,
            val_loader,
            epochs=epochs_count
        )

        if max(val_acc) > best_accuracy:
            best_accuracy = max(val_acc)

        curr_params = {'lr': lr, 'momentum': momentum, 'M_b': M_b, 'weight_decay': weight_decay, 'k': k, 'N': N}
        history.append({'params': curr_params, 'train_accuracies': train_acc, 'val_accuracies': val_acc, "cur_max_val_acc": best_accuracy})

    # Use the basic dictionary lambda/key sort feature
    history.sort(key=lambda x: x['cur_max_val_acc'], reverse=True)

    print("Top 3 part b models")
    for pos,config in enumerate(history[:3]):
        print(f" {pos+1}: {config['params']}, Val Acc: {config['cur_max_val_acc']:.3f}")

    return history[:3]

# Start running Part B code
iterations_count = 4
# "Use a coarser search space: If you are testing 100 different model configurations, it’s possible that it’s unnecessary, especially when you use momentum."

top_3 = test_parameters_partB(iterations_count, EPOCHS_HYPERPARAMETER_SEARCH)

if top_3:
  plot_training_results(top_3, "Part B", 0.65)
  # Now we will run it over the ENTIRE test set
  best_partb_result = top_3[0]
  best_partb_params = best_partb_result['params']

  print(f"Best Config: {best_partb_params}")
  print(f"Best Config Val Acc: {best_partb_result['cur_max_val_acc']:.3f}")


  model = get_partb_model(input_dim=c, M_b=best_partb_params['M_b'], k=best_partb_params['k'], N=best_partb_params['N'], img_size=w, output_dim=10)

  optimizer = SGD(model.parameters(), lr=best_partb_params['lr'], momentum=best_partb_params['momentum'], weight_decay=best_partb_params['weight_decay'])

  train(model, optimizer, train_loader, val_loader, EPOCHS_FINALTRAINING)

  test_loss, test_acc = evaluate(model, test_loader)

  # Retrain fully!
  print(f"Part B - Best Config Test Acc: {test_acc:.3f}")

else:
  print("Check your code, no results")